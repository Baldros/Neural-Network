{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMi+NFXjCUhWX+Ao4y1si9a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Baldros/Neural-Networks/blob/main/RNN_MIT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Apresentação:\n",
        "\n",
        "Saber lidar com **dados sequênciais** é uma habilidade importantíssima para um cientista de dados. O objetivo desse código então é, baseado na aula do **MIT** da professora [Ava Amini](https://www.mit.edu/~asolei/), que trata de modelos de **Redes Neurais** para **séries temporais**, construir do zero um modelo **Recurrent Neural Networks (RNN's)** para tradução e geração de música.\n",
        "\n",
        "## Material:\n",
        "* [MIT 6.S191: Recurrent Neural Networks, Transformers, and Attention\n",
        "](https://www.youtube.com/watch?v=dqoEU9Ac3ek)\n",
        "\n",
        "* [Capítulo 10: Sequence Modeling: Recurrentand Recursive Nets (Goodfellow)](https://www.deeplearningbook.org/contents/rnn.html)\n",
        "\n"
      ],
      "metadata": {
        "id": "YEGMpY2sZmfA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Biblioteca utilizada:\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "OMOx_nXsZ5JP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recurrent Neural Networks (RNN):\n",
        "\n",
        "## Modelando Sequências: Critérios de projeto\n",
        "\n",
        "Para modelas sequência, precisamos de:\n",
        "\n",
        "1. lidar com sequências de comprimento variável;\n",
        "\n",
        "2. Trackear dependencias de longo prazo;\n",
        "\n",
        "3. Manter a informação em ordem;\n",
        "\n",
        "4. Compartilhar os parâmetros para toda a sequência.\n",
        "\n",
        "E uma **RNN** trata de todas essas questões.\n",
        "\n",
        "## Intuição do Modelo:\n",
        "\n",
        "A ideia básica é construir um modelo cujas as camadas ocultas (_hidden layers_) dependam dos estados ocultos das etapas anteriores. Ou seja, a saída $\\hat{y}_t$\n",
        "depende do estado oculto anterior $h_{t-1}$ (memoria passada) e do input atual $x_t$ (presente), sendo $x_t \\in \\mathbb{R}^m$ e $\\hat{y}_t \\in \\mathbb{R}^n$, de modo que,\n",
        "\n",
        "$$\\hat{y}_t= f(x_t, h_{t-1})$$\n",
        "\n",
        "**RNN's** possuem um estado $h_t$ que é atualizado a cada passo a medida que a sequência é processada. Apricando a **Relação de Recorrencia** para cada passo de de processamento da sequência, temos:\n",
        "\n",
        "$$h_t = f_W(x_t, h_{t-1})$$\n",
        "\n",
        "Onde:\n",
        "\n",
        "* $h_t$: É o estado da célula;;\n",
        "* $f_w$: É a função com os pesso $W$;\n",
        "* $x_t$: vetor de entradas;\n",
        "* $h_{t-1}$: Estado anterior da célula.\n",
        "\n",
        "É importante dizer que a mesma função e conjunto de parâmetros são usados para cada passo do aprendizado."
      ],
      "metadata": {
        "id": "R_pF0pChfG2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Função de Predição:\n",
        "def predict(sentence, model):\n",
        "  '''\n",
        "  Função que realiza a predição do modelo.\n",
        "  '''\n",
        "\n",
        "  # Estruturando as entradas e as hidden layers:\n",
        "  list_of_words = sentence.split() # Splitando as palavras\n",
        "  hidden_state = [0]*len(list_of_words)\n",
        "\n",
        "  # Feedforward:\n",
        "  for word in list_of_words:\n",
        "    prediction, hidden_state = model(word,hidden_state)\n",
        "\n",
        "  return [prediction, hidden_state]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1G3FobLre9E-",
        "outputId": "59e03a37-5c91-44eb-b407-3dadc4fba211"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Esse é o core do processo, vamos agroa construir o modelo etapa por etapa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Atualização dos estados e saídas:\n",
        "\n",
        "Vimos que a ideia aqui é trabalhar com camadas ocultas dependentes, tal que $\\hat{y}_t= f(x_t, h_{t-1})$. Refinando então essa ideia, temos que,\n",
        "\n",
        "$$h_t =\\text{tanh}(W_{hh}^Th_{t-1}+W_{xh}^Tx_t)$$\n",
        "\n",
        "Deste modo então podemos definir a nossa saída, que se faz de forma que,\n",
        "\n",
        "$$\\hat{y}_t = W_{hy}^Th_t$$\n",
        "\n",
        "## Computando a perda:\n",
        "\n",
        "Essa Rede é baseada em grafos e como ja dito, a ideia aqui é reutilizar a mesma matriz de pesos para cada passo do aprendizado.\n",
        "\n",
        "A ideia aqui é computar a perda para cada etapa do treinamento e somar, obtendo a perda total do modelo."
      ],
      "metadata": {
        "id": "gE290EuwnLV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyRNNCell(tf.keras.layers.Layer):\n",
        "  def __init__(self, rnn_units,input_dim, output_dim):\n",
        "    super(MyRNNCell, self).__init__()\n",
        "\n",
        "    # Inicializando Matrizes de peso:\n",
        "    self.W_xh = self.add_weight([rnn_units, input_dim])\n",
        "    self.W_hh = self.add_weight([rnn_units, rnn_units])\n",
        "    self.W_hy = self.add_weight([output_dim, rnn_units])\n",
        "\n",
        "    # Instanciando estados ocultos:\n",
        "    self.h = tf.zeros([rnn_units,1])\n",
        "\n",
        "  def forwardpass(self, x):\n",
        "    # Atualização das camadas ocultas:\n",
        "    self.h = tf.math.tanh(self.W_hh*self.h + self.W_xh*x)\n",
        "\n",
        "    # Computando a saída:\n",
        "    output = self.W_hy*self.h\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "g4KP5pd5mtSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelando Sequências:"
      ],
      "metadata": {
        "id": "6yL1IaaYvPIm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "292IJuW-xM7X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
